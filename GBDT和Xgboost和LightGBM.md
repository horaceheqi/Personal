AdaBoost
---
原始的Adaboost算法开始的时候，为每一个样本赋上一个权重值，初始的时候，大家都是一样重要。在每一步训练中得到模型，会使得数据点的估计有对错，我们就在每一步结束后，增加分错点的权重，减少分对点的权重，这样使得某些点如果老师被分错，那么就会被"重点关注"，也就被赋一个很高的权重。然后等进行了N次迭代(由用户决定)，将会得到N个简单的分类器(basic learner)，然后我们将它们组合起来(加权、投票等等)得到一个最终模型

GBDT
---
GBDT(Gradient Boosting Decision Tree)中的树都是回归树，GBDT用来做回归预测，调整后也可以用于分类(设定阈值，大于阈值为正例，反之为负例)，可以发现多种有区分性的特征以及特征组合。GBDT是把所有树的结论累加起来做最终结论的，GBDT的核心就在于每一颗树学的是之前所有树结论和的残差(负梯度)，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁，那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二课树真能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在一岁的残差，第三棵树A的年龄就变成了1岁，继续学，Boosting的最大好处在于每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0，这样后面的树就能越来越专注哪些前面被分错的instance。
Gradient Boost优缺点
-优点：它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。
-缺点：Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征。

Xgboost
---
不同的机器学习模型适用于不同类型的任务。深度神经网络通过对时空位置建模，能够很好地捕获图像、语音、文本等高纬度数据。而基于树模型的Xgboost则能够很好地处理表格数据，同时还拥有一些深度神经网络所没有的特性(如：模型的可解释性、输入数据的不变性，更易于调参等)
XGBoost能自动利用cpu的多线程，而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度，

-XGBoost能自动利用cpu的多线程,而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度
-传统GBDT以CART作为基分类器，特指梯度提升决策树算法，而XGBoost还支持线性分类器(gblinear)，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）
-Xgboost 专注于模型的可解释性，而基于神经网路的深度学习，则更关注模型的准确度。
-Xgboost 更适用于变量数量较少的表格数据，而深度学习则更适用于图像或其他拥有海量变量的数据。


Gradient Boost与AdaBoost的区别
---
-Gradient Boost 的每一次计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少。Shrinkage(缩减)的思想认为。每次走一小步逐渐逼近结果的效果，要比每次迈大一步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。
-Adaboost是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”
